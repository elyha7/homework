{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid     category                subcategory  \\\n",
       "0  10000010    Транспорт      Автомобили с пробегом   \n",
       "1  10000094  Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299  Личные вещи     Детская одежда и обувь   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "\n",
       "                                               attrs   price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...    1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...    3000        NaN   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[0:3]\n",
    "#df.get('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 274996\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.sum()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "k,n=(0,0)\n",
    "fordrop=[]\n",
    "#< downsample data so that both classes have approximately equal ratios>\n",
    "for index, row in df.iterrows():\n",
    "    if row['is_blocked']==0:\n",
    "        fordrop.append(k)\n",
    "        n+=1\n",
    "    k+=1\n",
    "    if n==654957:\n",
    "        break\n",
    "#print fordrop\n",
    "df = df.drop(df.index[fordrop])\n",
    "\n",
    "\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "fordrop,k,n=(0,0,0)\n",
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "#if very_low_ram:\n",
    "#    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2367c4b310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "min_count = 10\n",
    "tokens=[]\n",
    "for i in token_counts.keys():\n",
    "    if token_counts[i]>=min_count:\n",
    "        tokens.append(i)\n",
    "#tokens = <tokens from token_counts keys that had at least min_count occurences throughout the dataset>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88036"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 88037\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [17036 14694 55439 82210 80290 17368     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8387     0 30504     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [28865 23455     0  3649 33977     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "for cat_str, subcat_str in df[[\"category\",\"subcategory\"]].values:\n",
    "    \n",
    "    cat_dict = {\"category\":cat_str,\"subcategory\":subcat_str}\n",
    "    categories.append(cat_dict)\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "data_split=train_test_split(title_tokens,desc_tokens,df_non_text.values,target)\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_split,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_split = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_split\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "from lasagne.nonlinearities import *\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412494\n",
      "412494\n"
     ]
    }
   ],
   "source": [
    "print len(target_tr)\n",
    "print len(desc_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer(shape=(None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer(shape=(None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer(shape=(None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn,num_units=128,nonlinearity=tanh,\n",
    "                                    grad_clipping=15)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn,num_units=256,nonlinearity=tanh,\n",
    "                                    only_return_final=True,grad_clipping=15)\n",
    "#RNN or LSTM over embedding, maybe several ones in a stack\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn,num_units=128,\n",
    "                                    nonlinearity=tanh,grad_clipping=15)#,only_return_final=True)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn,num_units=256,nonlinearity=tanh,\n",
    "                                    only_return_final=True,grad_clipping=15)\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=156,nonlinearity=tanh)\n",
    "#cat_nn = lasagne.layers.LSTMLayer(cat_nn,num_units=64,nonlinearity='rectify')\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_nn,num_units=312,nonlinearity=tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.ConcatLayer([descr_nn,title_nn,cat_nn])                               \n",
    "\n",
    "nn_dense = lasagne.layers.DenseLayer(nn,512,nonlinearity=leaky_rectify)\n",
    "nn = lasagne.layers.DropoutLayer(nn_dense,p=0.35)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "#accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_tr).mean()\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1).mean()\n",
    "#reg_l2=lasagne.regularization.regularize_layer_params_weighted(layers,l2)\n",
    "#reg_l1=lasagne.regularization.regularize_layer_params(layers,l1)*1e-4\n",
    "#loss=loss+reg_l1+reg_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1).mean()\n",
    "#det_reg_l2=lasagne.regularization.regularize_layer_params_weighted(layers,l2)\n",
    "#det_reg_l1=lasagne.regularization.regularize_layer_params(layers,l1)*1e-4\n",
    "#det_loss=loss+det_reg_l1+det_reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:36,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.171275269868\n",
      "\tacc: 0.920265780731\n",
      "\tauc: 0.959767183005\n",
      "\tap@k: 0.96111152222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.171227113721\n",
      "\tacc: 0.918637873754\n",
      "\tauc: 0.96987458328\n",
      "\tap@k: 0.914700487172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:37,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.166114880329\n",
      "\tacc: 0.922325581395\n",
      "\tauc: 0.959774518545\n",
      "\tap@k: 0.974665471563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.170555320504\n",
      "\tacc: 0.918837209302\n",
      "\tauc: 0.968759737784\n",
      "\tap@k: 0.928418206162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:40,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.158224182896\n",
      "\tacc: 0.925813953488\n",
      "\tauc: 0.962332595588\n",
      "\tap@k: 0.963368554475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.175847994492\n",
      "\tacc: 0.916478405316\n",
      "\tauc: 0.969475464088\n",
      "\tap@k: 0.961381513818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:38,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.160225805942\n",
      "\tacc: 0.925282392027\n",
      "\tauc: 0.958437473475\n",
      "\tap@k: 0.964287549997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.172567631923\n",
      "\tacc: 0.918205980066\n",
      "\tauc: 0.969397896981\n",
      "\tap@k: 0.921239500848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:43,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.160492330311\n",
      "\tacc: 0.925016611296\n",
      "\tauc: 0.960329582343\n",
      "\tap@k: 0.96581326675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.169316404582\n",
      "\tacc: 0.918737541528\n",
      "\tauc: 0.969850848601\n",
      "\tap@k: 0.982923487997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:45,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.158776743793\n",
      "\tacc: 0.926810631229\n",
      "\tauc: 0.962160552185\n",
      "\tap@k: 0.966003081511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.201130043511\n",
      "\tacc: 0.905448504983\n",
      "\tauc: 0.965282525489\n",
      "\tap@k: 0.988616743767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:39,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.148528192499\n",
      "\tacc: 0.931594684385\n",
      "\tauc: 0.966679399693\n",
      "\tap@k: 0.988376327656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.161668915106\n",
      "\tacc: 0.922558139535\n",
      "\tauc: 0.971512235826\n",
      "\tap@k: 0.961703060741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:35,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.154225125554\n",
      "\tacc: 0.928405315615\n",
      "\tauc: 0.963091923331\n",
      "\tap@k: 0.972538144592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.156387189217\n",
      "\tacc: 0.925249169435\n",
      "\tauc: 0.973363379399\n",
      "\tap@k: 0.993549904327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:34,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.152782336629\n",
      "\tacc: 0.929534883721\n",
      "\tauc: 0.962576622011\n",
      "\tap@k: 0.977764045836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.161251969466\n",
      "\tacc: 0.922159468439\n",
      "\tauc: 0.972509951611\n",
      "\tap@k: 0.996115690089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [03:45,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.142441980478\n",
      "\tacc: 0.933920265781\n",
      "\tauc: 0.964410686898\n",
      "\tap@k: 0.984886027083\n",
      "Val:\n",
      "\tloss: 0.159347708086\n",
      "\tacc: 0.923488372093\n",
      "\tauc: 0.972877240133\n",
      "\tap@k: 0.988497171808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 300\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in tqdm(enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True))):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.17373324716\n",
      "\tacc: 0.916761280932\n",
      "\tauc: 0.968103154297\n",
      "\tap@k: 0.941457323712\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tВы побили baseline (ok)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
